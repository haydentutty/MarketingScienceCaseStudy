{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobs_scraper import JobsScraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping in progress...: 100%|██████████| 3/3 [00:00<00:00, 17.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's create a new JobsScraper object and perform the scraping for a given query.\n",
    "scraper = JobsScraper(country=\"nl\", position=\"Data Engineer\", location=\"Amsterdam\", pages=3)\n",
    "df = scraper.scrape()\n",
    "print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "  \n",
    "  \n",
    "# user define function\n",
    "# Scrape the data\n",
    "# and get in string\n",
    "def getdata(url):\n",
    "    r = requests.get(url)\n",
    "    return r.text\n",
    "  \n",
    "# Get Html code using parse\n",
    "def html_code(url):\n",
    "  \n",
    "    # pass the url\n",
    "    # into getdata function\n",
    "    htmldata = getdata(url)\n",
    "    soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "  \n",
    "    # return html code\n",
    "    return(soup)\n",
    "  \n",
    "# filter job data using\n",
    "# find_all function\n",
    "def job_data(soup):\n",
    "    \n",
    "    # find the Html tag\n",
    "    # with find()\n",
    "    # and convert into string\n",
    "    data_str = \"\"\n",
    "    for item in soup.find_all(\"a\", class_=\"jobtitle turnstileLink\"):\n",
    "        data_str = data_str + item.get_text()\n",
    "    result_1 = data_str.split(\"\\n\")\n",
    "    return(result_1)\n",
    "  \n",
    "# filter company_data using\n",
    "# find_all function\n",
    "  \n",
    "  \n",
    "def company_data(soup):\n",
    "  \n",
    "    # find the Html tag\n",
    "    # with find()\n",
    "    # and convert into string\n",
    "    data_str = \"\"\n",
    "    result = \"\"\n",
    "    for item in soup.find_all(\"div\", class_=\"sjcl\"):\n",
    "        data_str = data_str + item.get_text()\n",
    "    result_1 = data_str.split(\"\\n\")\n",
    "  \n",
    "    res = []\n",
    "    for i in range(1, len(result_1)):\n",
    "        if len(result_1[i]) > 1:\n",
    "            res.append(result_1[i])\n",
    "    return(res)\n",
    "  \n",
    "  \n",
    "# driver nodes/main function\n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    # Data for URL\n",
    "    job = \"data+science+internship\"\n",
    "    Location = \"Noida%2C+Uttar+Pradesh\"\n",
    "    url = \"https://in.indeed.com/jobs?q=\"+job+\"&l=\"+Location\n",
    "  \n",
    "    # Pass this URL into the soup\n",
    "    # which will return\n",
    "    # html string\n",
    "    soup = html_code(url)\n",
    "  \n",
    "    # call job and company data\n",
    "    # and store into it var\n",
    "    job_res = job_data(soup)\n",
    "    com_res = company_data(soup)\n",
    "  \n",
    "    # Traverse the both data\n",
    "    temp = 0\n",
    "    for i in range(1, len(job_res)):\n",
    "        j = temp\n",
    "        for j in range(temp, 2+temp):\n",
    "            print(\"Company Name and Address : \" + com_res[j])\n",
    "  \n",
    "        temp = j\n",
    "        print(\"Job : \" + job_res[i])\n",
    "        print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jobs_from(website, job_title, location, desired_characs, filename= \"test.csv\"):    \n",
    "    \"\"\"\n",
    "    This function extracts all the desired characteristics of all new job postings\n",
    "    of the title and location specified and returns them in single file.\n",
    "    The arguments it takes are:\n",
    "        - Website: to specify which website to search (options: 'Indeed' or 'CWjobs')\n",
    "        - Job_title\n",
    "        - Location\n",
    "        - Desired_characs: this is a list of the job characteristics of interest,\n",
    "            from titles, companies, links and date_listed.\n",
    "        - Filename: to specify the filename and format of the output.\n",
    "            Default is .xls file called 'results.xls'\n",
    "    \"\"\"\n",
    "    \n",
    "    if website == 'Indeed':\n",
    "        job_soup = load_indeed_jobs_div(job_title, location)\n",
    "        jobs_list, num_listings = extract_job_information_indeed(job_soup, desired_characs)\n",
    "    \n",
    "    if website == 'CWjobs':\n",
    "        # TO DO LATER\n",
    "    \n",
    "        save_jobs_to_excel(jobs_list, filename)\n",
    " \n",
    "def extract_job_information_indeed(job_soup, desired_characs):\n",
    "    job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
    "     \n",
    "    cols = []\n",
    "    extracted_info = []\n",
    "    \n",
    "    \n",
    "    if 'titles' in desired_characs:\n",
    "        titles = []\n",
    "        cols.append('titles')\n",
    "        for job_elem in job_elems:\n",
    "            titles.append(extract_job_title_indeed(job_elem))\n",
    "        extracted_info.append(titles)                    \n",
    "    \n",
    "    if 'companies' in desired_characs:\n",
    "        companies = []\n",
    "        cols.append('companies')\n",
    "        for job_elem in job_elems:\n",
    "            companies.append(extract_company_indeed(job_elem))\n",
    "        extracted_info.append(companies)\n",
    "    \n",
    "    if 'links' in desired_characs:\n",
    "        links = []\n",
    "        cols.append('links')\n",
    "        for job_elem in job_elems:\n",
    "            links.append(extract_link_indeed(job_elem))\n",
    "        extracted_info.append(links)\n",
    "    \n",
    "    if 'date_listed' in desired_characs:\n",
    "        dates = []\n",
    "        cols.append('date_listed')\n",
    "        for job_elem in job_elems:\n",
    "            dates.append(extract_date_indeed(job_elem))\n",
    "        extracted_info.append(dates)\n",
    "    \n",
    "    jobs_list = {}\n",
    "    \n",
    "    for j in range(len(cols)):\n",
    "        jobs_list[cols[j]] = extracted_info[j]\n",
    "    \n",
    "    num_listings = len(extracted_info[0])\n",
    "    \n",
    "    return jobs_list, num_listings\n",
    "    print('{} new job postings retrieved. Stored in {}.'.format(num_listings, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'RemoteJobs'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request, send_file\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "def get_so_jobs(term):\n",
    "  job_list = []\n",
    "  url = f\"https://stackoverflow.com/jobs?r=true&q={term}\"\n",
    "  html_doc = requests.get(url).text\n",
    "  soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "  list_results = soup.find(\"div\", {\"class\":\"listResults\"})\n",
    "  head_soup = list_results.find_all(\"h2\", {\"class\":\"mb4 fc-black-800 fs-body3\"})\n",
    "  for head in head_soup:\n",
    "    header = head.find(\"a\")\n",
    "    link = \"https://stackoverflow.com\" + header.get(\"href\")\n",
    "    title = header.get_text()\n",
    "    job_list.append([title, \"company_name\", link])\n",
    "  comp_soup = list_results.find_all(\"h3\", {\"class\":\"fc-black-700 fs-body1 mb4\"})\n",
    "  for i, company in enumerate(comp_soup):\n",
    "    comp_name = company.find(\"span\").text.strip()\n",
    "    job_list[i][1] = comp_name\n",
    "\n",
    "  return job_list\n",
    "\n",
    "\n",
    "def get_wwr_jobs(term):\n",
    "  job_list = []\n",
    "  url = f\"https://weworkremotely.com/remote-jobs/search?term={term}\"\n",
    "  html_doc = requests.get(url).text\n",
    "  soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "  list_soup = soup.find(\"ul\")\n",
    "  list_item = list_soup.find_all(\"li\", {\"class\":\"feature\"})\n",
    "  for info in list_item:\n",
    "    title = info.find(\"span\", {\"class\":\"title\"}).text\n",
    "    company = info.find(\"span\", {\"class\":\"company\"}).text\n",
    "    link = \"https://weworkremotely.com\" + info.find(\"a\").get(\"href\")\n",
    "    job_list.append([title, company, link])\n",
    "  \n",
    "  return job_list\n",
    "\n",
    "\n",
    "def get_remo_jobs(term):\n",
    "  job_list = []\n",
    "  url = f\"https://remoteok.io/remote-dev+{term}-jobs\"\n",
    "  html_doc = requests.get(url).text\n",
    "  soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "  tr_soup = soup.find_all(\"tr\", {\"class\":\"job\"})\n",
    "  for tr in tr_soup:\n",
    "    title = tr.find(\"h2\", {\"itemprop\":\"title\"}).text\n",
    "    company = tr.get(\"data-company\")\n",
    "    link = \"https://remoteok.io\" + tr.get(\"data-href\")\n",
    "    job_list.append([title, company, link])\n",
    "\n",
    "  return job_list\n",
    "\n",
    "\n",
    "def write_csv(job_list):\n",
    "  with open(\"jobs.csv\", \"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for job_info in job_list:\n",
    "      writer.writerow(job_info)\n",
    "    file.close()\n",
    "\n",
    "db= {}\n",
    "app = Flask(\"RemoteJobs\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "  return render_template(\"home.html\")\n",
    "\n",
    "@app.route(\"/search\")\n",
    "def search():\n",
    "  global job_list\n",
    "  search_key = request.args.get(\"term\").lower()\n",
    "  if search_key in db.keys():\n",
    "    job_list = db[search_key]\n",
    "  else:\n",
    "    job_list = get_so_jobs(search_key) + get_wwr_jobs(search_key) + get_remo_jobs(search_key)\n",
    "    db[search_key] = job_list\n",
    "\n",
    "  return render_template(\"search.html\", term=search_key, job_list=job_list, length=len(job_list))\n",
    "\n",
    "@app.route(\"/jobs.csv\")\n",
    "def export_file():\n",
    "  write_csv(job_list)\n",
    "  return send_file(\"jobs.csv\")\n",
    "\n",
    "\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('MyDemoEnv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5b8403d17c9fc93254a1e7719e9e5fb6b6b6a05282c0a6485e99fb2f2326f83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
